{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Optimization of Industrial Uptime: Predictive Maintenance Using GMMs and SVMs\n\n**Dataset:** AI4I 2020 Predictive Maintenance Dataset (UCI ML Repository)  \n**Module:** Data Analytics ECS784U/P  \n**Date:** February 2026\n\n---\n\n> **How to obtain the dataset**  \n> Download `ai4i2020.csv` from:  \n> https://archive.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset  \n> Place the CSV file in the same directory as this notebook before running.\n\n---\n\n## Table of Contents\n1. Data Loading and Initial Exploration\n2. Data Preprocessing\n3. Feature Selection\n4. Unsupervised Learning: K-Means and GMM Clustering\n5. Supervised Learning: SVM Classification\n6. Feature Reduction Using PCA\n7. Conclusions\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Data Loading and Initial Exploration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"OMP_NUM_THREADS\"] = '1'   # stops kmeans memory-leak warnings on some platforms\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nnp.set_printoptions(precision=3, suppress=True,\n                    formatter={'float': lambda x: f'{x:7.3f}'}, linewidth=100)\n\nprint(\"Libraries imported successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv('ai4i2020.csv')   # load the real dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.columns   # listing the dataframe columns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.dtypes   # returns the datatype of each column"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.shape   # returns the shape of data - rows and columns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.head()   # view the first few rows"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.describe()   # description of the numerical variables"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.info()   # variable types and missing value counts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# UDI and Product ID are unique identifiers - drop them before analysis\nprint('UDI has {} unique values - will drop'.format(len(df['UDI'].unique())))\nprint('Product ID has {} unique values - will drop'.format(len(df['Product ID'].unique())))\ndf = df.drop(['UDI', 'Product ID'], axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distributions of the categorical target variable\nprint('Values and counts for Machine failure are:')\nprint(df['Machine failure'].value_counts())\nprint()\nprint('Normalised counts for Machine failure are:')\nprint(df['Machine failure'].value_counts(normalize=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df['Machine failure'].value_counts().plot.bar(figsize=(5, 4),\n                                              title='Machine Failure Distribution')\nplt.xlabel('Machine failure (0=Normal, 1=Failure)')\nplt.ylabel('Count')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distributions of the categorical feature variable: Type\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\ndf['Type'].value_counts().plot(ax=axes[0], kind='bar',\n                               title='Product Type Distribution',\n                               color=['b', 'g', 'r'],\n                               ylabel='Count', xlabel='Type')\ndf['Machine failure'].value_counts().plot(ax=axes[1], kind='bar',\n                                          title='Machine Failure Distribution',\n                                          xlabel='Machine failure')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bivariate analysis: does Type predict Machine failure?\nType_status = pd.crosstab(df['Type'], df['Machine failure'])\nprint('Counts for Type vs Machine failure:')\nprint(Type_status)\n\n# Normalise so each row sums to 1 - shows the failure rate per product type\nType_status = Type_status.div(Type_status.sum(1).astype(float), axis=0)\nprint('\\nNormalised counts for Type vs Machine failure:')\nprint(Type_status)\n\nType_status.plot(kind='bar', stacked=True, figsize=(6, 4),\n                 xlabel='Product Type', ylabel='Fraction',\n                 title='Machine Failure Rate by Product Type')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distributions of continuous variables (histogram, density, boxplot)\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\ndf['Torque [Nm]'].plot(kind='hist', bins=50, ax=axes[0],\n                       xlabel='Torque [Nm]', title='Histogram')\ndf['Torque [Nm]'].plot(kind='density', color='r', ax=axes[1], title='Density Plot')\ndf['Torque [Nm]'].plot(kind='box', ax=axes[2], ylabel='Torque [Nm]',\n                       xlabel='', title='Boxplot')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bivariate analysis: does Rotational speed differ across failure status?\ndf.plot(column='Rotational speed [rpm]', by='Machine failure', kind='box',\n        subplots=False, xlabel='Machine failure', ylabel='Rotational speed [rpm]',\n        title='Rotational Speed by Failure Status', figsize=(5, 4))\nplt.xlabel('Machine failure')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mean of each continuous variable grouped by failure status\ndf.groupby('Machine failure')['Torque [Nm]'].mean().plot(\n    kind='bar', ylabel='Mean Torque [Nm]', figsize=(5, 4),\n    title='Mean Torque by Failure Status')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Preprocessing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Missing Value Imputation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count missing values per column\ndf.isnull().sum()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fill missing values in numerical columns with the mean\nfor col in df.select_dtypes(include=[np.number]).columns:\n    df[col] = df[col].fillna(df[col].mean())\n\n# Confirm no missing values remain\nprint('Missing values after imputation:')\nprint(df.isnull().sum())\n\n# Save a copy of the data after imputation for use in the PCA section later\ndf_original = df.copy()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Log Transformations for Skewed Distributions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rotational speed and Torque both have skewed distributions\ndf['Rotational_Speed_Log'] = np.log(df['Rotational speed [rpm]'])\ndf['Torque_Log'] = np.log(df['Torque [Nm]'])\ndf['Tool_Wear_Log'] = np.log(df['Tool wear [min]'].replace(0, 1))\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\ndf['Rotational speed [rpm]'].plot(kind='hist', bins=50, ax=axes[0],\n                                  title='Raw: Rotational Speed Distribution')\ndf['Rotational_Speed_Log'].plot(kind='hist', bins=50, ax=axes[1],\n                                title='Log: Rotational Speed Distribution')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Engineered Features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Temperature difference: linked to Heat Dissipation Failure\ndf['Temp_Diff'] = df['Process temperature [K]'] - df['Air temperature [K]']\n\n# Power: torque * angular velocity (omega = 2*pi*rpm/60)\ndf['Power'] = df['Torque [Nm]'] * (2 * np.pi * df['Rotational speed [rpm]'] / 60)\n\n# Strain: tool wear * torque (linked to Overstrain Failure)\ndf['Strain'] = df['Tool wear [min]'] * df['Torque [Nm]']\n\nprint('Engineered features added: Temp_Diff, Power, Strain')\ndf[['Temp_Diff', 'Power', 'Strain']].describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.4 Convert Categorical Variables to Integer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert the Type column (L, M, H) to integer codes\nprint('Type is originally a string variable:')\nprint(df['Type'].value_counts())\n\ndf['Type'] = df['Type'].astype('category').cat.codes\nprint('\\nType has been converted to integer:')\nprint(df['Type'].value_counts())\n\n# Drop the raw failure sub-mode columns to prevent data leakage\n# (TWF, HDF, PWF, OSF, RNF directly determine Machine failure)\ndf = df.drop(['TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1)\n\nprint('\\nData frame after encoding and dropping leakage columns:')\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Feature Selection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1 Correlation Heatmap"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "corr = df.corr()\nplt.figure(figsize=(14, 6))\nsns.heatmap(corr, annot=True, cmap='BuPu')\nplt.title('Correlation Matrix of All Features')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Drop highly correlated raw features that are superseded by engineered equivalents\n# Rotational speed and Torque are captured in Power and their log transforms\ncols_to_drop = ['Air temperature [K]', 'Process temperature [K]']\ndf = df.drop(columns=cols_to_drop, axis=1)\n\nprint('Remaining features after dropping highly correlated columns:')\nprint(df.columns.tolist())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 SelectKBest and ExtraTreesClassifier Feature Importance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_classif\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Split into feature matrix X and target vector y\nX = df.drop(['Machine failure'], axis=1)\ny = df['Machine failure']\n\n# Chi-squared test (requires non-negative values - shift negative columns)\nX_pos = X - X.min()\n\nchi2_sel = SelectKBest(score_func=chi2, k='all').fit(X_pos, y)\nchi2_sorted = pd.Series(data=chi2_sel.scores_, index=X.columns).sort_values()\n\n# F-test\nftest = SelectKBest(score_func=f_classif, k='all').fit(X, y)\nftest_sorted = pd.Series(data=ftest.scores_, index=X.columns).sort_values()\n\n# Mutual information\nmitest = SelectKBest(score_func=mutual_info_classif, k='all').fit(X, y)\nmitest_sorted = pd.Series(data=mitest.scores_, index=X.columns).sort_values()\n\n# ExtraTreesClassifier\nxtrees = ExtraTreesClassifier(random_state=42).fit(X, y)\nxtrees_sorted = pd.Series(data=xtrees.feature_importances_, index=X.columns).sort_values()\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\nplt.subplots_adjust(wspace=0.6)\nchi2_sorted.plot(kind='barh', ax=axes[0, 0], title='Using chi2 score')\nftest_sorted.plot(kind='barh', ax=axes[0, 1], title='Using FTest score')\nxtrees_sorted.plot(kind='barh', ax=axes[1, 1], title='Using ExtraTreesClassifier')\nmitest_sorted.plot(kind='barh', ax=axes[1, 0], title='Using MI Test score')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Unsupervised Learning: K-Means and GMM Clustering\n\nFollowing the Lab 5 Part 2 case study approach with `kmeans()` and `gmm()` wrapper functions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Prepare and Scale Features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\n# Use the engineered features most relevant to failure modes\ncluster_features = ['Torque [Nm]', 'Tool wear [min]', 'Power', 'Temp_Diff', 'Strain']\nX_cluster = df[cluster_features].copy()\n\nscaler_cluster = MinMaxScaler().fit(X_cluster)\nX_scaled = scaler_cluster.transform(X_cluster)\nX_scaled = pd.DataFrame(X_scaled, columns=cluster_features)\nprint('Scaled data shape:', X_scaled.shape)\nX_scaled.describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Wrapper Functions (Lab 5 Part 2 pattern)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_kmeans(X, n_clusters):\n    \"\"\"\n    Use KMeans to group data into a specified number of clusters.\n    Returns (inertia, cluster_centers, labels)\n    \"\"\"\n    model = KMeans(n_clusters=n_clusters, init='k-means++',\n                   max_iter=200, n_init=10, random_state=0)\n    model.fit(X)\n    return (model.inertia_, model.cluster_centers_, model.labels_)\n\n\ndef run_gmm(X, n_clusters):\n    \"\"\"\n    Use GMM to group data into a specified number of clusters.\n    Returns (BIC_score, means, labels)\n    \"\"\"\n    model = GaussianMixture(n_components=n_clusters, random_state=123, n_init=5)\n    model.fit(X)\n    score = model.bic(X)\n    labels = model.predict(X)\n    return (score, model.means_, labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Elbow Method: Choosing Number of Clusters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "kbest_scores = []\ngmm_scores = []\n\nfor i in range(1, 11):\n    score, centres, labels = run_kmeans(X_scaled, i)\n    sizes = pd.Series(labels).value_counts().to_dict()\n    print('KMeans has {} clusters with sizes {} with score {:.2f}'\n          .format(i, sizes, score))\n    kbest_scores.append(score)\n\n    score, centres, labels = run_gmm(X_scaled, i)\n    sizes = pd.Series(labels).value_counts().to_dict()\n    print('GMM has {} clusters with sizes {} with score {:.2f}\\n'\n          .format(i, sizes, score))\n    gmm_scores.append(score)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "figure, axis = plt.subplots(1, 2, figsize=(14, 5))\naxis[0].plot(range(1, 11), kbest_scores)\naxis[0].scatter(3, kbest_scores[2], s=200, c='red', marker='*')\naxis[0].set_title('The Elbow Method - KMeans')\naxis[0].set_xlabel('Number of clusters')\naxis[0].set_ylabel('Clustering Score (Inertia)')\n\naxis[1].plot(range(1, 11), gmm_scores)\naxis[1].scatter(3, gmm_scores[2], s=200, c='red', marker='*')\naxis[1].set_title('The Elbow Method - GMM')\naxis[1].set_xlabel('Number of clusters')\naxis[1].set_ylabel('Clustering Score (BIC)')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 Analyse Cluster Characteristics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def list_clusters(method, X, n_clusters, scaler, failure_labels):\n    \"\"\"\n    Print cluster sizes, failure rates, and rescaled cluster centres.\n    failure_labels: array of Machine failure values (0/1) aligned with X rows.\n    \"\"\"\n    score, centres, labels = method(X, n_clusters)\n    sizes = pd.Series(labels).value_counts().to_dict()\n\n    # Compute failure rate per cluster\n    failure_rate = {}\n    for label in range(n_clusters):\n        mask = labels == label\n        rate = failure_labels[mask].mean()\n        failure_rate[label] = round(rate, 4)\n\n    print('\\nThere are {} clusters with a total score of {:.1f}\\n'\n          .format(len(sizes), score))\n\n    for label, centre in enumerate(centres):\n        # Rescale centres back to original units\n        centre_rescaled = {\n            X.columns[i]: round(centre[i] * scaler.data_range_[i]\n                                + scaler.data_min_[i], 2)\n            for i in range(len(centre))\n        }\n        print('Cluster {} has {} posts, failure rate={}, centre:\\n{}\\n'\n              .format(label, sizes.get(label, 0),\n                      failure_rate.get(label, 'n/a'), centre_rescaled))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "failure_arr = df['Machine failure'].values\nlist_clusters(run_kmeans, X_scaled, 3, scaler_cluster, failure_arr)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "list_clusters(run_gmm, X_scaled, 3, scaler_cluster, failure_arr)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.5 PCA Visualisation of Clusters in 2D Space"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.decomposition import PCA\n\n_, _, kmeans_labels = run_kmeans(X_scaled, 3)\n_, _, gmm_labels = run_gmm(X_scaled, 3)\n\nX_PCA = PCA(2).fit_transform(X_scaled)\n\nkwargs = dict(cmap=plt.colormaps['Set1'], edgecolor='none', alpha=0.6)\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nax[0].scatter(X_PCA[:, 0], X_PCA[:, 1], c=kmeans_labels, **kwargs)\nax[0].set_title('3 KMeans clusters\\nplotted in 2 component space')\nax[1].scatter(X_PCA[:, 0], X_PCA[:, 1], c=gmm_labels, **kwargs)\nax[1].set_title('3 GMM clusters\\nplotted in 2 component space')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Supervised Learning: SVM Classification\n\nFollowing the Lab 5 Part 1 pattern: `train_and_evaluate()` helper, then hyperparameter tuning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.svm import SVC\n\ndef train_and_evaluate(model, X, y):\n    \"\"\"\n    Train and evaluate a classification model using cross-validation,\n    then report metrics on a held-out test set.\n    \"\"\"\n    print('\\nResults from algorithm {}:'.format(model))\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y)\n\n    # 5-fold cross-validation accuracy on training data\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n    print('Mean cross-validation F1 is {:.3f} with SD {:.3f}'\n          .format(np.mean(scores), np.std(scores)))\n\n    learnt_model = model.fit(X_train, y_train)\n    print('\\nF1 on training data is {:.3f}\\n'.format(\n        f1_score(y_train, learnt_model.predict(X_train))))\n\n    y_pred = model.predict(X_test)\n    print('Test data metrics: accuracy={:.3f}, f1={:.3f}, precision={:.3f}, recall={:.3f}'\n          .format(accuracy_score(y_test, y_pred),\n                  f1_score(y_test, y_pred),\n                  precision_score(y_test, y_pred),\n                  recall_score(y_test, y_pred)))\n\n    cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    plt.figure(figsize=(3, 3))\n    ax = sns.heatmap(cm, annot=True, xticklabels=['Normal', 'Failure'],\n                     yticklabels=['Normal', 'Failure'],\n                     cbar=False, square=True, linewidths=4.0)\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    return learnt_model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scale feature matrix for SVM\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_svm = df.drop(['Machine failure'], axis=1)\ny_svm = df['Machine failure']\n\nscaler_svm = MinMaxScaler()\nX_svm_scaled = scaler_svm.fit_transform(X_svm)\nX_svm_scaled = pd.DataFrame(X_svm_scaled, columns=X_svm.columns)\n\nprint('Feature matrix shape for SVM:', X_svm_scaled.shape)\nprint('Class balance:')\nprint(y_svm.value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SVM with linear kernel (Lab 4 pattern: simplest case first)\n_ = train_and_evaluate(SVC(kernel='linear', class_weight='balanced'), X_svm_scaled, y_svm)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SVM with RBF kernel\n_ = train_and_evaluate(SVC(kernel='rbf', C=1.0, gamma='scale',\n                            class_weight='balanced'), X_svm_scaled, y_svm)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1 Hyperparameter Tuning: Finding Best C"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_model(algorithm, hyperparams, X, y):\n    \"\"\"\n    Use cross-validation to evaluate a model with given hyperparameters.\n    Returns (mean_f1, trained_model).\n    \"\"\"\n    model = algorithm(**hyperparams)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n    learnt_model = model.fit(X_train, y_train)\n    return (np.mean(scores), learnt_model)\n\n\n# Try C values 0.1, 0.2, ... 2.0\nc_values = [0.1 * i for i in range(1, 21)]\nc_f1 = []\nfor c in c_values:\n    score, _ = train_model(SVC, {'kernel': 'rbf', 'C': c, 'gamma': 'scale',\n                                  'class_weight': 'balanced'},\n                           X_svm_scaled, y_svm)\n    c_f1.append(score)\n\nplt.plot(c_values, c_f1)\nplt.xlabel('Value of regularisation hyperparameter C')\nplt.ylabel('Mean cross-validation F1 on training data')\nplt.title('SVM RBF - F1 vs C')\nplt.show()\n\nbest_c = c_values[np.argmax(c_f1)]\nprint(f'Best C = {best_c:.1f} with mean F1 = {max(c_f1):.3f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the best model on the held-out test set\nprint(f'Final evaluation using best C = {best_c}')\nX_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n    X_svm_scaled, y_svm, test_size=0.2, random_state=42, stratify=y_svm)\n\nbest_svm = SVC(kernel='rbf', C=best_c, gamma='scale', class_weight='balanced')\nbest_svm.fit(X_train_f, y_train_f)\ny_pred_f = best_svm.predict(X_test_f)\n\nprint('Test data metrics: accuracy={:.3f}, f1={:.3f}, precision={:.3f}, recall={:.3f}'\n      .format(accuracy_score(y_test_f, y_pred_f),\n              f1_score(y_test_f, y_pred_f),\n              precision_score(y_test_f, y_pred_f),\n              recall_score(y_test_f, y_pred_f)))\n\ncm = confusion_matrix(y_true=y_test_f, y_pred=y_pred_f)\nplt.figure(figsize=(3, 3))\nax = sns.heatmap(cm, annot=True, xticklabels=['Normal', 'Failure'],\n                 yticklabels=['Normal', 'Failure'],\n                 cbar=False, square=True, linewidths=4.0)\nax.set_xlabel('Predicted')\nax.set_ylabel('Actual')\nplt.title(f'Best SVM (C={best_c})')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Feature Reduction Using PCA\n\nFollowing the Lab 5 Part 1 pattern: revert to the post-imputation data, \nuse `LabelEncoder` + `MinMaxScaler`, plot cumulative explained variance, \nthen compare SVM accuracy with different numbers of components."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\n\n# Start from the saved post-imputation copy, remove leakage columns\ndf_pca = df_original.copy()\ndf_pca = df_pca.drop(['TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1)\n\n# Encode Type using LabelEncoder (Lab 5 Part 1 approach)\nle = LabelEncoder()\ndf_pca['Type'] = le.fit_transform(df_pca['Type'])\n\nX_pca = df_pca.drop(['Machine failure'], axis=1)\ny_pca = df_pca['Machine failure']\n\nprint(X_pca.head())\n\n# Scale\nscaler_pca = MinMaxScaler()\nscaler_pca.fit(X_pca)\nX_pca_scaled = scaler_pca.transform(X_pca)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot cumulative explained variance for 1 to all components\npca_full = PCA(n_components=X_pca_scaled.shape[1]).fit(X_pca_scaled)\nn = X_pca_scaled.shape[1]\n\nplt.plot(range(1, n + 1), np.cumsum(pca_full.explained_variance_ratio_))\nplt.xticks(range(1, n + 1))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title('PCA: Cumulative Explained Variance')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate SVM with 4 PCA components\npca4 = PCA(n_components=4).fit(X_pca_scaled)\nX_reduc4 = pca4.transform(X_pca_scaled)\nprint('\\nPCA reduces features from {} to {}'.format(X_pca_scaled.shape, X_reduc4.shape))\n_ = train_and_evaluate(SVC(kernel='rbf', C=best_c, gamma='scale',\n                            class_weight='balanced'), X_reduc4, y_pca)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate SVM with 6 PCA components\npca6 = PCA(n_components=6).fit(X_pca_scaled)\nX_reduc6 = pca6.transform(X_pca_scaled)\nprint('\\nPCA reduces features from {} to {}'.format(X_pca_scaled.shape, X_reduc6.shape))\n_ = train_and_evaluate(SVC(kernel='rbf', C=best_c, gamma='scale',\n                            class_weight='balanced'), X_reduc6, y_pca)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Conclusions\n\n### Key Findings\n\n1. **Data Exploration**: The AI4I 2020 dataset is heavily imbalanced (~3.4% failure rate). \n   Product Type shows limited standalone predictive power for failure, but engineered features \n   (Power, Strain, Temp_Diff) derived from the physical failure mechanisms are more informative.\n\n2. **Unsupervised Learning (GMM / K-Means)**: The elbow method suggests 3 clusters as a \n   reasonable choice. Cluster centres, once rescaled to original units, allow meaningful \n   interpretation of operating regimes and their associated failure rates.\n\n3. **Supervised Learning (SVM)**: The RBF kernel with `class_weight='balanced'` handles the \n   class imbalance effectively. Hyperparameter tuning via cross-validation identified the best \n   regularisation strength C; recall is the most critical metric for a maintenance use case \n   (missing a failure is more costly than a false alarm).\n\n4. **PCA for Dimensionality Reduction**: The cumulative explained variance plot identifies a \n   natural elbow point. Reducing to that number of components retains most predictive information \n   while decreasing the feature space, though full-feature SVM generally outperforms PCA-reduced \n   versions on this dataset.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('=' * 60)\nprint('ANALYSIS COMPLETE')\nprint('=' * 60)\nprint(f'Dataset loaded: ai4i2020.csv')\nprint(f'Total samples: {df_original.shape[0]}')\nprint(f'Failure rate: {df_original[\"Machine failure\"].mean()*100:.2f}%')\nprint(f'Features used for SVM: {X_svm_scaled.shape[1]}')\nprint(f'Best SVM C value found: {best_c}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}